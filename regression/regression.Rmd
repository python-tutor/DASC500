---
title: "Untitled"
author: "Jason Freels"
date: "12/28/2019"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(reticulate)
knitr::knit_engines$set(python = reticulate::eng_python)
```

```{python import}
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
```

Applied Review

`scikit-learn` ( abbreviated in Python as `sklearn`) is the general machine learning package in Python's Data Science Ecosystem

Python has a lot of other statistics-oriented packages to meet various data science needs, including `scipy` and `statsmodels`

Machine Learning Overview

scikit-learn is a package for machine learning

You may hear this called modeling, predictive analytics, data mining, artificial intelligence, etc. But machine learning is a catch-all term for these things

Despite being broad, most machine learning methodologies and problems fit into two categories: supervised learning and unsupervised learning

## Supervised Learning

Generally speaking, supervised learning problems are focused on predicting/estimating/labeling a truth

Another way to think of this is learning a function -- given the inputs, attempt to predict/estimate/label the output

Examples:

- Estimating a customer's satisfaction with a product they haven't tried before
- Predicting next month's sales
- Predicting the outcome of a sporting event
- Labeling whether a photo contains a cucumber or a zucchini

All of these examples of supervised learning fit into two distinct categories: regression and classification

### Regression

Regression is focused on estimating/predicting a continuous output

Question: Which of our examples are regression problems?
Estimating a customer's satisfaction with a product they haven't tried before
Predicting next month's sales
Predicting the outcome of a sporting event
Labeling whether a photo contains a cucumber or a zucchini

While regression sounds like linear regression, there are a variety of algorithms for regression:

- Linear
- Polynomial
- Ridge
- Lasso
- ElasticNet
- Decision Trees
- Random Forest
- Gradient Boosting
- Neural Networks

### Classification

Classification is focused on predicting/labeling a discrete output (categories)

Our other two examples are classification problems:
Predicting the outcome of a sporting event
Labeling whether a photo contains a cucumber or a zucchini

Note that there can be two (yes/no) or more categories

Just like regression, classification has a variety of algorithms:

Regression - Logistic, Multinomial, Ordinal, Logit, Probit
- Nearest Neighbors
- Decision Trees
- Random Forest
- Gradient Boosting
- Neural Networks

## Unsupervised Learning

While supervised learning problems are focused on predicting a truth, unsupervised learning has no truth data

In other words, there is no output -- therefore, there's no function to learn

Instead, unsupervised learning is focused on learning patterns between cases/observations

### Clustering

Clustering is focused on grouping observations into categories based on similarities -- these categories are not set or known beforehand

An example of this is a customer segmentation -- placing customers in segments based on their buying patterns

A few key clustering algorithms:
KMeans
Hierarchical
Spectral

### Dimension Reduction

Dimension reduction is focused on reducing the number of dimensions needed to represent an observation or case

An example is taking an observation with 150 variables and representing its variability in just 10 variables -- this is helpful in avoiding the curse of dimensionality and collinearity

A few key dimension reduction algorithms:
Principle Components Analysis
Non-negative Matrix Factorization

### Text Vectorization

While text mining and natural language processing are their own branch of data science, text vectorization is an example of unsupervised learning

Text vectorization is focused on creating numeric representations of text by learning from words and their surroundings

These methods are increasingly popular -- they're used heavily in chatbots and digital assistants

The two common algorithms are `Word2Vec` and `Doc2Vec`

Building a Supervised Learning Regression Model

We are going to go through an example of building a linear regression model in Python using sklearn

While other algorithms may be more predictive, this is a good exercise to start with sklearn

We are going to cover some basics of applied machine learning throughout the exercise

## Setting Up the Problem

A predictive model isn't going to fix any problems that haven't already been defined

Let's say that we want to try to predict whether or not a flight is going to be delayed

Let's begin by importing the flights data set

```{python flights_df}
import pandas as pd
flights_df = pd.read_csv('data/flights.csv')
```

Let's review our data to see the target and what features we have available

```{python flights_df_head}
flights_df.head(3)
```

Our target variable is dep_delay

Question: Which features may be useful?

month, day, sched_dep_time, sched_arr_time, carrier, origin, distance

We want to be sure to avoid leakage -- this is when you include information that would not be available at the time of prediction
dep_time, arr_time, air_time, etc.

Our final target and feature specification can be viewed visually below:

` Image `

Preparing Data

There are a variety of things that need to be done with our data prior to modeling

While the model may be the exciting part, the uniquely human skills (setting up the problem, data prep, etc.) are where our time is usually best spent

Data Exploration

The first thing we should do is explore our data

While there are a variety of assumptions we should validate for linear regression, we're just going to check a few things:
Remove missing values
The distribution of our target variable
The data types of our feature variables

Because we are using linear regression, our data can't have any missing values

While we could investigate interpolation to fill them in, we're just going to remove them

```{python flights_df_dropna}
flights_df = flights_df[['dep_delay',
                         'month',
                         'day', 
                         'sched_dep_time',
                         'sched_arr_time', 
                         'carrier',
                         'origin',
                         'distance']]
                         
flights_df = flights_df.dropna()
```

We will next review the distribution of our target variable

```{python describe}
flights_df['dep_delay'].describe()
```

And we can also visualize this

```{python dp2}
dp2 = sns.distplot(flights_df['dep_delay']);
plt.show(dp2)
#fig.savefig('images/dp2.png', bbox_inches='tight')
```

As we can see, our data has some large outliers -- we're going to remove these from our analysis so they don't bias the model

```{python quantile}
flights_df['dep_delay'].quantile(0.99)
```

The 99th percentile is 191, so we'll remove anything above that

```{python dp3}
flights_df = flights_df.loc[flights_df['dep_delay'] < flights_df['dep_delay'].quantile(0.99)]

dp3 = sns.distplot(flights_df['dep_delay'].dropna());

plt.show(dp3);
```

Next, we're going to look at the data types of our possible feature variables

```{python flights_df_dtypes}
flights_df[['month', 
            'day',
            'sched_dep_time',
            'sched_arr_time',
            'carrier', 
            'origin', 
            'distance']].dtypes
```

We can see that all of our variables but two, carrier and origin, are of a numeric type

This is good, because linear regression requires numeric inputs

Question: What do we do with our categorical variables?

Feature Engineering

Feature engineering is the practice of using domain knowledge of the data to create features that make machine learning algorithms work

Examples:
Making sure data are the correct types
Applying transfomation techniques
Combining features to account for interaction effects
Using domain knowledge to combine features in intelligent ways

For now, we're just going to focus on making sure our data are the correct types -- and we need to convert carrier and origin to numeric variables

```{python carrier_origin}
flights_df[['carrier', 'origin']].head()
```

There are a variety of ways we can encode our categorical variables into numeric variables: dummy encoding, target encoding, etc.

We are going to focus on a form of dummy encoding called one-hot encoding

One-hot encoding is the process of creating a binary/dummy variable for all but one of the possible categories within a categorical variable

This will work because we have limited categories for our carrier and origin variables

```{python carrier_origin_count}
flights_df[['carrier']].drop_duplicates().count()

flights_df[['origin']].drop_duplicates().count()
```

There's a one-hot encoding function called get_dummies() that's built into pandas because it's so popular

```{python get_dummies}
flights_df = pd.get_dummies(flights_df)
flights_df.head(3)
```

Note that there are many functionalities like this built into sklearn, but the `get_dummies()` function is really intuitive

Data Partitioning

Next we need to partition our data into a training set and a test/validation set

Question: Why might we want to split our data into two different sets?

When an algorithm is trained on data to learn a function, it can learn all of the patterns in that data

But some of that those patterns are just noise -- patterns that are random and specific to the data used to train the model

This results in a biased model that doesn't generalize well to new data for which predictions are wanted -- this is called overfitting

Combatting Overfitting

The reason we partition our data is to prevent overfitting

We train the model on one data set, and then test or validate the model on another data set

If the model predicts well on the test set that was not used to train it, it can be assumed that it generalizes well

Partitioning Data

We can use the train_test_split() function from sklearn's model_selection module to split our data frame into a training data set and a testing data set

```{python tt_split}
from sklearn.model_selection import train_test_split
train_flights_df, test_flights_df = train_test_split(flights_df, 
                                                     test_size = 0.2)
```

Notice that we unpacked the result via multiple assignment -- both train_flights_df and test_flights_df were assigned simultaneously

We can verify that these train and test sets are the appropriate sizes

```{python tt_shapes}
train_flights_df.shape

test_flights_df.shape
```

And it's also a good idea to make sure there are similar distributions in the target variable

```{python dp4}
plt.show(sns.distplot(train_flights_df['dep_delay']))
```
```{python dp5}
plt.show(sns.distplot(test_flights_df['dep_delay']))
```


Note that there are other forms of data partitioning (i.e. cross validation, k-fold, leave-one-out, etc.) that are useful and beneficial, but we don't have time to cover them here

Training the Model

Next we are actually going to train the model using sklearn

This is where the model learns the actual patterns in the data

First, let's import the LinearRegression() module

```{python import_lregression}
from sklearn.linear_model import LinearRegression
```

We have to start by instantiating our LinearRegression object -- this is common practice in computer science

```{python lregression1}
linear_model = LinearRegression()
```

Next, we can split our target variable and features to feed into linear_model

```{python}
train_flights_df_features = train_flights_df[[
    feature for feature in train_flights_df.columns if feature != 'dep_delay'
]]

train_flights_df_target = train_flights_df['dep_delay']
```

Now we can train linear_model by calling the fit() method and passing in our data

```{python lregression_model}
linear_model.fit(X = train_flights_df_features, 
                 y = train_flights_df_target)
                 
LinearRegression(copy_X = True, 
                 fit_intercept = True, 
                 n_jobs = None, 
                 normalize = False)
```

Now our model is built, and we need to validate it.

Validating the Model

There are many different ways to validate a linear regression that are specific to its statistical properties

We are going to focus on algorithm-agnostic validation techniques that can be used on other algorithms

Two common methods are to validate a supervised learning model are error metrics and variable-model relationships

Error Metrics

There are a variety of error metics we can use to measure the performance of the predictive model

Many of these focus on comparing the actual target values to the predicted target values

We well use root mean squared error (RMSE) -- it's a common choice because:
It captures an average quantity of error
It more heavily penalizes really bad predictions

Calculating RMSE

We will begin by importing the necessary functions -- we could do this by hand, but sklearn makes life easier

```{python sqrt}
from sklearn.metrics import mean_squared_error
from math import sqrt
```

Next, we'll compute the RMSE for the training data -- this can serve as a benchmark to see if we overfit the data

```{python train_rmse}
train_rmse = sqrt(mean_squared_error(train_flights_df_target,
                  linear_model.predict(train_flights_df_features)))
                  
train_rmse
```

Next we'll do the same for the test data, but we need to do the preprocessing again, as well

```{python features}
features = [feature for feature in test_flights_df.columns if feature != 'dep_delay']
features


test_flights_df_features = test_flights_df[features]
test_flights_df_target = test_flights_df['dep_delay']
test_flights_df_features = test_flights_df[features]
test_flights_df_target = test_flights_df['dep_delay']
```

And then we can compute the RMSE

```{python test_rmse}
test_rmse = sqrt(mean_squared_error(test_flights_df_target, linear_model.predict(test_flights_df_features)))
test_rmse
```

We see there doesn't seem to be overfitting, which is good

Question: Is this RMSE good?

Comparing to Target

It can be helpful to see how this RMSE (which can be thought of as a weighted average error), compares to the target variable's standard deviation

This can help put into context reason relative magnitude of the error

```{python print_rmse}
print('Test RMSE:', test_rmse)
print('Test Target Standard Deviation', sqrt(test_flights_df_target.var()))
```

In addition, it's very common to create a predicted vs. actual plot to visually see how each prediction compares to the actual

```{python scatterplot}
sns.scatterplot(test_flights_df_target,
                linear_model.predict(test_flights_df_features), 
                alpha = 0.01).set(xlim = (-20, 200), ylim=(-20, 200))
                
sns.regplot(test_flights_df_target,
            linear_model.predict(test_flights_df_features), 
            scatter = False);
```

While there is a slight directional impact, this model is not very predictive

That's okay, modeling is an iterative project -- there are ways to improve, and the best thing to do is look at your data

Variable Considerations

For a linear regression, each feature has a trained coefficient a representation of how it impacts the prediction

It will helpful to know which coefficients impact our predictions -- let's make a function to do this

```{python get_coefficients}
def get_coefficients(columns, model_object):
    '''Function to sort get the sorted coefficients of a linear model'''
    import operator
    dictionary = dict(zip(columns, model_object.coef_))
    return sorted(dictionary.items(), key = operator.itemgetter(1))

get_coefficients(test_flights_df_features.columns, linear_model)
```

Question: Are there any feature coefficients here that are surprising?

I would think that the month would be more predictive -- why isn't it?

Month is an integer data type from 1-12 -- does the weather follow a constant change pattern from months 1-12?

We can likely make our model more predictive by treating each month as a category and one-hot encoding it

```{python flights_df2}
flights_df['month'] = flights_df['month'].astype(str)
flights_df = pd.get_dummies(flights_df)
flights_df.head()
```

Then we can rebuild our model

```{python use_model}
train_flights_df, test_flights_df = train_test_split(flights_df, 
                                                     test_size = 0.2)
                                                     
linear_model = LinearRegression()
features = [feature for feature in train_flights_df.columns if feature != 'dep_delay']

train_flights_df_features = train_flights_df[features]
train_flights_df_target = train_flights_df['dep_delay']
linear_model.fit(X = train_flights_df_features, y = train_flights_df_target)
```

And validate the results

```{python train_rmse2}
train_rmse = sqrt(mean_squared_error(train_flights_df_target, linear_model.predict(train_flights_df_features)))
train_rmse
```

```{python test_rmse2}
test_flights_df_features = test_flights_df[[feature for feature in test_flights_df.columns if feature != 'dep_delay']]
test_flights_df_target = test_flights_df['dep_delay']
test_flights_df_features = test_flights_df[[feature for feature in test_flights_df.columns if feature != 'dep_delay']]
test_flights_df_target = test_flights_df['dep_delay']

test_rmse = sqrt(mean_squared_error(test_flights_df_target, linear_model.predict(test_flights_df_features)))
test_rmse

sns.scatterplot(test_flights_df_target,
                linear_model.predict(test_flights_df_features), 
                alpha = 0.01).set(xlim = (-20, 200), ylim=(-20, 200))
                
sns.regplot(test_flights_df_target, 
            linear_model.predict(test_flights_df_features), 
            scatter = False);
```

Our results got marginally better, but a lot more could be done

Question: What else could we do to improve our predictions?

More feature engineering (day/night flights, day-of-week, holidays, etc.)

Collect more data (weather forecast, incoming flight information*, estimated security times, plane capacity, number of passengers, plane manufacturer and model, etc.)

Choose a different algorithm...

Keep doing these things...and know when to stop

Learning More

Firstly, don't feel intimated -- you're not going to learn this in an hour

There are a lot of things you can do to improve your skills

Books
Introduction to Statistical Learning or Elements of Statistical Learning, Hastie, Tibshirani, and Friedman
Python Data Science Handbook, Jake VanderPlas
Hands-on Machine Learning with scikit-learn and TensorFlow, Aurélien Géron

Online Courses
Machine Learning with Python - Coursera

Practice
Use your own data
Kaggle

Questions

## Lasso Regression

```{python import_lasso}
from sklearn.linear_model import Lasso
```

# Use the default lasso hyperparameters

```{python use_lasso}
lasso = Lasso()

lasso.fit(train_flights_df_features, 
          train_flights_df_target)
```

Are there any questions before moving on?